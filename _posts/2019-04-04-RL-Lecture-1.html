---
layout: post
title: "RL Lecture Notes: Sequential Decision Making"
---

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async>
</script>

<div class="post">

<blockquote>Reinforcement Learning - Lecture 1 - Emma Brunskill</blockquote>

<h3>Learning to Make Good Sequential Decisions</h3>

<p>Key aspects of reinforcement learning:</p>

<ul>
<li><strong>Optimization</strong>: Finding policies that maximize expected reward</li>
<li><strong>Exploration</strong>: Balancing exploration vs exploitation</li>
<li><strong>Generalization</strong>: Transferring knowledge to new situations</li>
<li><strong>Delayed Consequences</strong>: Decisions have long-term ramifications</li>
</ul>

<h3>Challenges</h3>

<p><strong>When planning</strong>: Decisions involve reasoning about not just immediate benefit but also longer-term ramifications.</p>

<p><strong>When learning</strong>: Temporal credit assignment is hard - what caused later high or low rewards?</p>

<h3>Policy</h3>

<p>A policy is a mapping from past experience to action.</p>

<h3>Comparison with Other Paradigms</h3>

<p><strong>Supervised Learning</strong>: Typically making one decision instead of a sequence of decisions.</p>

<p><strong>Imitation Learning</strong>: Learns from experience of others, assumes input demos of good policies. Imitation + RL seems promising.</p>

<h3>Sequential Decision Making Under Uncertainty</h3>

<p>Goal: Maximize the total expected future reward (the world is stochastic so the agent maximizes rewards in expectation).</p>

<h3>The Markov Assumption</h3>

<p>State \(s_t\) is Markov if and only if:</p>

$$p(s_{t+1} | s_t, a_t) = p(s_{t+1} | h_{t}, a_{t})$$

<p>The current state is a sufficient statistic of history.</p>

<h3>Problem Variants</h3>

<ul>
<li>Finite horizon vs. infinite horizon</li>
<li>Stationary vs. non-stationary</li>
</ul>

</div>
