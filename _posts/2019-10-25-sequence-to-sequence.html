---
layout: post
title: Sequence to Sequence Learning with Neural Networks
---

<div class="post">

<blockquote>Using LSTMs for general sequence-to-sequence problems</blockquote>

<p>Deep neural networks are powerful machine learning models that achieve excellent performance on difficult problems. If there exists a parameter setting of a large DNN that achieves good results, supervised backpropagation will find these parameters and solve the problem.</p>

<h3>The Problem</h3>

<p>Many important problems are best expressed with sequences whose lengths are not known a-priori. For example, speech recognition and machine translation are sequential problems. Likewise, question-answering can also be seen as mapping a sequence of words representing the question to a sequence of words representing the answer.</p>

<h3>The Approach</h3>

<p>The core idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain a large fixed-dimensional vector representation, and then use another LSTM to extract the output sequence from that vector.</p>

<p>The second LSTM is essentially a recurrent neural network language model, except that it is conditioned on the input sequence.</p>

<h3>Key Properties</h3>

<ul>
<li>The LSTM learns to map an input sequence of variable length into a fixed-dimensional vector representation</li>
<li>The encoder-decoder architecture separates input processing from output generation</li>
<li>Attention mechanisms (in later work) address limitations of fixed-size representations</li>
</ul>

<h3>Related Work</h3>

<ul>
<li>Kalchbrenner and Blunsom, "Recurrent continuous translation models" (EMNLP, 2013)</li>
<li>Cho et al., "Learning phrase representations using RNN encoder-decoder for statistical machine translation" (2014)</li>
</ul>

</div>
